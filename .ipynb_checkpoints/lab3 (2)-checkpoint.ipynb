{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\appu2\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-56f13afbc7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-56f13afbc7a6>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mfeatures_nd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_to_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mtrain_then_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_nd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-56f13afbc7a6>\u001b[0m in \u001b[0;36mtrain_then_build_model\u001b[1;34m(data_labels, features_nd)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     X_train1, X_test1, y_train1, y_test1  = train_test_split(\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mdata_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "   \n",
    "\n",
    "def load_data():\n",
    "    data = []\n",
    "\n",
    "    data_labels = []\n",
    "    with open(\"pos_tweets.txt\",encoding=\"utf8\") as f:\n",
    "        for i in f: \n",
    "            data.append(i) \n",
    "            data_labels.append('pos')\n",
    "\n",
    "    with open(\"neg_tweets.txt\",encoding=\"utf8\") as f:\n",
    "        for i in f: \n",
    "            data.append(i)\n",
    "            data_labels.append('neg')\n",
    "\n",
    "    return data, data_labels\n",
    "\n",
    "def transform_to_features(data):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer = 'word',\n",
    "        lowercase = False,\n",
    "    )\n",
    "    features = vectorizer.fit_transform(\n",
    "        data\n",
    "    )\n",
    "    features_nd = features.toarray()\n",
    "    return features_nd\n",
    "\n",
    "def train_then_build_model(data_labels, features_nd):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)\n",
    "    \n",
    "    X_train1, X_test1, y_train1, y_test1  = train_test_split(\n",
    "        data, \n",
    "        data_labels,\n",
    "        train_size=0.80, \n",
    "        random_state=1234)\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log_model = LogisticRegression()\n",
    "\n",
    "    log_model = log_model.fit(X=X_train, y=y_train)\n",
    "    y_pred = log_model.predict(X_test)\n",
    "\n",
    "   \n",
    "    for i in range(10):\n",
    "        print(\":: {0} :: {1}\".format(y_pred[i],X_test1[i]))\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    a=sklearn.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
    "    print(\"Accuracy={}\".format(a))\n",
    "\n",
    "def process():\n",
    "    data, data_labels = load_data()\n",
    "    features_nd = transform_to_features(data)\n",
    "    train_then_build_model(data_labels, features_nd)\n",
    "\n",
    "\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
